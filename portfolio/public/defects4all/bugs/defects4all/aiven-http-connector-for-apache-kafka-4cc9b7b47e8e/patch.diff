diff --git a/build.gradle b/build.gradle
index 20c783e..bbbe5f2 100644
--- a/build.gradle
+++ b/build.gradle
@@ -99,6 +99,7 @@ dependencies {
     }
     integrationTestImplementation "io.confluent:kafka-avro-serializer:$confluentPlatformVersion"
     integrationTestImplementation "io.confluent:kafka-connect-avro-converter:$confluentPlatformVersion"
+    integrationTestImplementation "io.confluent:kafka-json-serializer:$confluentPlatformVersion"
     integrationTestImplementation "org.apache.avro:avro:1.8.1"
     integrationTestImplementation "org.apache.kafka:connect-json:$kafkaVersion"
     integrationTestImplementation "org.apache.kafka:connect-transforms:$kafkaVersion"
diff --git a/src/integration-test/java/io/aiven/kafka/connect/http/JsonIntegrationTest.java b/src/integration-test/java/io/aiven/kafka/connect/http/JsonIntegrationTest.java
new file mode 100644
index 0000000..2d45efb
--- /dev/null
+++ b/src/integration-test/java/io/aiven/kafka/connect/http/JsonIntegrationTest.java
@@ -0,0 +1,210 @@
+/*
+ * Copyright 2021 Aiven Oy and http-connector-for-apache-kafka project contributors
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package io.aiven.kafka.connect.http;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+
+import org.apache.kafka.clients.admin.AdminClient;
+import org.apache.kafka.clients.admin.AdminClientConfig;
+import org.apache.kafka.clients.admin.NewTopic;
+import org.apache.kafka.clients.producer.KafkaProducer;
+import org.apache.kafka.clients.producer.ProducerConfig;
+import org.apache.kafka.clients.producer.ProducerRecord;
+import org.apache.kafka.clients.producer.RecordMetadata;
+
+import io.aiven.kafka.connect.http.mockserver.BodyRecorderHandler;
+import io.aiven.kafka.connect.http.mockserver.MockServer;
+
+import com.fasterxml.jackson.annotation.JsonProperty;
+import org.junit.jupiter.api.AfterEach;
+import org.junit.jupiter.api.BeforeAll;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.Timeout;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.testcontainers.containers.KafkaContainer;
+import org.testcontainers.containers.Network;
+import org.testcontainers.junit.jupiter.Container;
+import org.testcontainers.junit.jupiter.Testcontainers;
+
+
+import static org.junit.jupiter.api.Assertions.assertIterableEquals;
+
+@Testcontainers
+public class JsonIntegrationTest {
+    private static final Logger log = LoggerFactory.getLogger(IntegrationTest.class);
+
+    private static final String HTTP_PATH = "/send-data-here";
+    private static final String AUTHORIZATION = "Bearer some-token";
+    private static final String CONTENT_TYPE = "application/json";
+
+    private static final String CONNECTOR_NAME = "test-source-connector";
+
+    private static final String TEST_TOPIC = "test-topic";
+    private static final int TEST_TOPIC_PARTITIONS = 1;
+
+    static final String JSON_PATTERN = "{\"record\":\"%s\",\"recordValue\":\"%s\"}";
+
+    private static File pluginsDir;
+
+    @Container
+    private final KafkaContainer kafka = new KafkaContainer()
+            .withNetwork(Network.newNetwork())
+            .withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false");
+
+    private AdminClient adminClient;
+    private KafkaProducer<String, Record> producer;
+
+    private ConnectRunner connectRunner;
+
+    private MockServer mockServer;
+
+    @BeforeAll
+    static void setUpAll() throws IOException, InterruptedException {
+        final File testDir = Files.createTempDirectory("http-connector-for-apache-kafka-").toFile();
+        testDir.deleteOnExit();
+
+        pluginsDir = new File(testDir, "plugins/");
+        assert pluginsDir.mkdirs();
+
+        // Unpack the library distribution.
+        final File transformDir = new File(pluginsDir, "http-connector-for-apache-kafka/");
+        assert transformDir.mkdirs();
+        final File distFile = new File(System.getProperty("integration-test.distribution.file.path"));
+        assert distFile.exists();
+        final String cmd = String.format("tar -xf %s --strip-components=1 -C %s",
+                distFile, transformDir);
+        final Process p = Runtime.getRuntime().exec(cmd);
+        assert p.waitFor() == 0;
+    }
+
+    @BeforeEach
+    void setUp() throws ExecutionException, InterruptedException {
+        mockServer = new MockServer(HTTP_PATH, AUTHORIZATION, CONTENT_TYPE);
+
+        final Properties adminClientConfig = new Properties();
+        adminClientConfig.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafka.getBootstrapServers());
+        adminClient = AdminClient.create(adminClientConfig);
+
+        final Map<String, Object> producerProps = Map.of(
+                ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka.getBootstrapServers(),
+                ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "io.confluent.kafka.serializers.KafkaJsonSerializer",
+                ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "io.confluent.kafka.serializers.KafkaJsonSerializer",
+                ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "1"
+        );
+        producer = new KafkaProducer<>(producerProps);
+
+        final NewTopic testTopic = new NewTopic(TEST_TOPIC, TEST_TOPIC_PARTITIONS, (short) 1);
+        adminClient.createTopics(List.of(testTopic)).all().get();
+        connectRunner = new ConnectRunner(pluginsDir, kafka.getBootstrapServers());
+        connectRunner.start();
+    }
+
+    @AfterEach
+    final void tearDown() {
+        connectRunner.stop();
+        adminClient.close();
+        producer.close();
+
+        connectRunner.awaitStop();
+
+        mockServer.stop();
+    }
+
+    @Test
+    @Timeout(30)
+    final void testBasicDelivery() throws ExecutionException, InterruptedException {
+        final BodyRecorderHandler bodyRecorderHandler = new BodyRecorderHandler();
+        mockServer.addHandler(bodyRecorderHandler);
+        mockServer.start();
+
+        connectRunner.createConnector(basicConnectorConfig());
+
+        final List<String> expectedBodies = new ArrayList<>();
+        final List<Future<RecordMetadata>> sendFutures = new ArrayList<>();
+        for (int i = 0; i < 1000; i++) {
+            for (int partition = 0; partition < TEST_TOPIC_PARTITIONS; partition++) {
+                final String key = "key-" + i;
+                final var recordName = "user-" + i;
+                final var recordValue = "value-" + i;
+                final Record value = new Record(recordName, recordValue);
+                expectedBodies.add(String.format(JSON_PATTERN, recordName, recordValue));
+                sendFutures.add(sendMessageAsync(partition, key, value));
+            }
+        }
+        producer.flush();
+
+        TestUtils.waitForCondition(() -> bodyRecorderHandler.recorderBodies().size() >= expectedBodies.size(),
+                15000,
+                "All requests received by HTTP server"
+        );
+        log.info("Recorded request bodies: {}", bodyRecorderHandler.recorderBodies());
+        assertIterableEquals(expectedBodies, bodyRecorderHandler.recorderBodies());
+
+        log.info("{} HTTP requests were expected, {} were successfully delivered",
+                expectedBodies.size(),
+                bodyRecorderHandler.recorderBodies().size());
+    }
+
+    private Map<String, String> basicConnectorConfig() {
+        final var config = new HashMap<String, String>();
+        config.put("name", CONNECTOR_NAME);
+        config.put("connector.class", HttpSinkConnector.class.getName());
+        config.put("topics", TEST_TOPIC);
+        config.put("key.converter", "org.apache.kafka.connect.json.JsonConverter");
+        config.put("value.converter", "org.apache.kafka.connect.json.JsonConverter");
+        config.put("tasks.max", "1");
+        config.put("key.converter.schemas.enable", "false");
+        config.put("value.converter.schemas.enable", "false");
+        config.put("http.url", "http://localhost:" + mockServer.localPort() + HTTP_PATH);
+        config.put("http.authorization.type", "static");
+        config.put("http.headers.authorization", AUTHORIZATION);
+        config.put("http.headers.content.type", CONTENT_TYPE);
+        return config;
+    }
+
+    private static class Record {
+        @JsonProperty
+        public String record;
+
+        @JsonProperty
+        public String recordValue;
+
+        public Record(final String record, final String recordValue) {
+            this.record = record;
+            this.recordValue = recordValue;
+        }
+    }
+
+    private Future<RecordMetadata> sendMessageAsync(final int partition,
+                                                    final String key,
+                                                    final Record value) {
+        final ProducerRecord<String, Record> msg = new ProducerRecord<>(
+                TEST_TOPIC, partition, key, value);
+        return producer.send(msg);
+    }
+}
diff --git a/src/main/java/io/aiven/kafka/connect/http/converter/RecordValueConverter.java b/src/main/java/io/aiven/kafka/connect/http/converter/RecordValueConverter.java
index 07e89f7..d92dea1 100644
--- a/src/main/java/io/aiven/kafka/connect/http/converter/RecordValueConverter.java
+++ b/src/main/java/io/aiven/kafka/connect/http/converter/RecordValueConverter.java
@@ -16,6 +16,7 @@
 
 package io.aiven.kafka.connect.http.converter;
 
+import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.kafka.connect.data.Struct;
@@ -24,9 +25,12 @@ import org.apache.kafka.connect.sink.SinkRecord;
 
 public class RecordValueConverter {
 
+    private final JsonRecordValueConverter jsonRecordValueConverter = new JsonRecordValueConverter();
+
     private final Map<Class<?>, Converter> converters = Map.of(
             String.class, record -> (String) record.value(),
-            Struct.class, new JsonRecordValueConverter()
+            HashMap.class, jsonRecordValueConverter,
+            Struct.class, jsonRecordValueConverter
     );
 
     interface Converter {
@@ -36,7 +40,8 @@ public class RecordValueConverter {
     public String convert(final SinkRecord record) {
         if (!converters.containsKey(record.value().getClass())) {
             throw new DataException(
-                    "Record value must be String or Schema Struct, but " + record.value().getClass() + " is given");
+                    "Record value must be String, Schema Struct or HashMap," 
+                    + " but " + record.value().getClass() + " is given");
         }
         return converters.get(record.value().getClass()).convert(record);
     }
